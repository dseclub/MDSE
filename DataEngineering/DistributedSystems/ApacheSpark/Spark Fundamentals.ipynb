{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-01T10:15:16.732374Z",
     "start_time": "2019-05-01T10:15:16.715142Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ultimate Spark Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Installation\n",
    "* Comparision with Pandas\n",
    "* Data Sources for I/O\n",
    "    * Connecting with Relational & NoN-Relational Databases\n",
    "* Deployment Modes\n",
    "    * spark-submit\n",
    "    * Local, Standalone, Cloud (AWS Demo)\n",
    "    * Running jupyter notebook\n",
    "* ML & Analytics - Notebook Demo\n",
    "    * Concept of PipeLines in Spark\n",
    "    * Estimators & Transformers\n",
    "    * Create a Machine Learning Model\n",
    "* Understanding Spark Internals - Notebook Demo\n",
    "    * Caching\n",
    "    * Repartitioning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 1 : Install Java, Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Install Scala and Java\n",
    "cd ~\n",
    "sudo apt install default-jre scala\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Step 2 : Download Spark and set SPARK_HOME in .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Download Spark\n",
    "wget https://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
    "tar xvf spark-2.4.0-bin-hadoop2.7.tgz\n",
    "sudo mv spark-2.4.0-bin-hadoop2.7 /usr/local/spark\n",
    "\n",
    "## put these lines in bashrc\n",
    "export SPARK_HOME=/usr/local/spark\n",
    "export PATH=$PATH:$SPARK_HOME/bin\n",
    "export JAVA_HOME=/usr/lib/jvm/default-java\n",
    "\n",
    "## refresh .bashrc file\n",
    "source .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## Test it\n",
    "pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quick Look - Compare with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark :\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('myfirst program')\\\n",
    "                            .master(\"local[4]\")\\\n",
    "                            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp = pd.read_csv(\"data/hotel_energy.csv\", \n",
    "                  header=0)\n",
    "\n",
    "# PySpark:\n",
    "dfs = spark.read.csv(\"data/hotel_energy.csv\", \n",
    "                     header=True, \n",
    "                     inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Show DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas : \n",
    "dfp.head(15)\n",
    "\n",
    "# PySpark :\n",
    "dfs.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Column and Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp.columns\n",
    "dfp.dtypes\n",
    "\n",
    "# PySpark :\n",
    "dfs.columns\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change Column Names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp.columns = [\"a\", \"b\", \"c\"]\n",
    "\n",
    "dfp.rename(columns = {\"old1\":\"new1\",\n",
    "                      \"old2\":\"new2\"})\n",
    "\n",
    "# PySpark : \n",
    "dfs1 = df.toDF(*[\"a\", \"b\", \"c\"])\n",
    "\n",
    "dfs.withColumnRenamed(\"old1\", \"new1\")\\\n",
    "   .withColumnRenamed(\"old2\", \"new2\")\\\n",
    "   .withColumnRenamed(\"old3\", \"new2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp.drop(\"hotel\", axis=1)\n",
    "\n",
    "# PySpark :\n",
    "dfs.drop(\"hotel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Change Column Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "dfp[\"sales\"].astype('int')\n",
    "\n",
    "\n",
    "# PySpark :\n",
    "from pyspark.sql.functions import col\n",
    "df = dfs.withColumn(\"sales\", col(\"sales\").cast(\"int\"))\n",
    "\n",
    "BinaryType: binary\n",
    "BooleanType: boolean\n",
    "ByteType: tinyint\n",
    "DateType: date\n",
    "DecimalType: decimal(10,0)\n",
    "DoubleType: double\n",
    "FloatType: float\n",
    "IntegerType: int\n",
    "LongType: bigint\n",
    "ShortType: smallint\n",
    "StringType: string\n",
    "TimestampType: timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "dfs = dfs.withColumn(\"sales\", col(\"sales\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aggregates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "df.groupby(['age', 'gender'])\\\n",
    "  .agg({'height':\"mean\", 'income':'min'})\n",
    "\n",
    "# Pyspark :    \n",
    "df.groupby(['age', 'gender'])\\\n",
    "  .agg({'height':\"mean\", 'income':'min'})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standard Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas :\n",
    "import numpy as np\n",
    "df['log_sales'] = np.log(df[\"sales\"])\n",
    "\n",
    "# Pyspark:\n",
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('log_sales', F.log(df.sales))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "# -\n",
    "\n",
    "# Pyspark:\n",
    "df.createOrReplaceTempView(\"df_VIEW\")\n",
    "ans_df = spark.sql(\"select * from df_VIEW where fruit == 'orange'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col,sum\n",
    "def missing_df(df):\n",
    "    df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Spark Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Spark Data Sources](https://i2.wp.com/www.jenunderwood.com/wp-content/uploads/2016/10/SparkArchitecture-Databrickss.gif?resize=800%2C462&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T19:03:49.414337Z",
     "start_time": "2019-03-14T19:03:49.405028Z"
    }
   },
   "source": [
    "# Data Ingestion From External Sources - Spark\n",
    "* Generic Format\n",
    "* HDFS vs Local File\n",
    "* Special Format - Need Drivers\n",
    "    * Avro\n",
    "    * S3\n",
    "* Relational Database\n",
    "    * Postgres\n",
    "    * MySQL\n",
    "* NoN-Relational Database\n",
    "    * Cassandra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Format - Dont need drivers\n",
    "\n",
    "* csv\n",
    "* json\n",
    "* parquet\n",
    "* libsvm\n",
    "* text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "\n",
    "spark.read.<format>(\"<file name>\")\n",
    "\n",
    "#### Write\n",
    "\n",
    "spark.write.<format>(\"<file name>\")\n",
    "\n",
    "#### read\n",
    "spark.read.format(\"<format name>\").options(\"path\", \"<path here>\").load()\n",
    "\n",
    "#### write\n",
    "spark.write.format(\"<format name>\").options(\"path\", \"<path here>\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS vs Local File\n",
    "\n",
    "There is a different convention when dealing with files stored in HDFS and Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T19:13:00.919492Z",
     "start_time": "2019-03-14T19:13:00.913293Z"
    }
   },
   "outputs": [],
   "source": [
    "#### HDFS\n",
    "spark.read.<format>.(\"hdfs://<full path here>\") # notice the double slash\n",
    "\n",
    "#### Local File \n",
    "spark.read.<format>(\"file:///<full path here>\") # notice the triple slash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special Formats - Need Drivers\n",
    "\n",
    "You can include the following packages using **--packages**\n",
    "\n",
    "|Source| Driver Package|\n",
    "|-----------|----------------|\n",
    "|S3        |org.apache.hadoop:hadoop-aws:2.7.1|\n",
    "|Avro       |org.apache.spark:spark-avro_2.11:2.4.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3\n",
    "\n",
    "Set up your Access Key and Scret Key in .bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export AWS_ACCESS_KEY_ID=<access ID here>\n",
    "    \n",
    "export AWS_SECRET_ACCESS_KEY=<access KEY here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "\n",
    "df = spark.read.<format>(\"s3a://<bucket name>/<file name>\") # notice the s3a\n",
    "\n",
    "#### Write\n",
    "\n",
    "df.write.<format>(\"s3a://<bucket name>/<file name>\", mode=\"overwrite\") # notice the s3a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Databases\n",
    "\n",
    "|Source| Driver Package|Driver Name|Standard Port|\n",
    "|-----------|----------------|---------|----|\n",
    "|Postgres   |org.postgresql:postgresql:42.1.1|org.postgresql.Driver|5432\n",
    "|MySQL       |mysql:mysql-connector-java:8.0.13|com.mysql.jdbc.Driver|3306\n",
    "|SQL Server| Download from internet - see SQLserver section| com.mysql.jdbc.Driver|  1433|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Generic Read\n",
    "\n",
    "spark.read\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"<driver name>\")\\\n",
    "      .option(\"url\", \"jdbc:<source>://<ip>:<port>/<dbname>\")\\\n",
    "      .option(\"dbtable\", \"<table>\")\\\n",
    "      .option(\"user\", \"<username>\")\\\n",
    "      .option(\"password\",\"<password>\")\\\n",
    "      .load()\n",
    "\n",
    "#### Generic Write\n",
    "\n",
    "df.write\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_demo_db\")\\\n",
    "      .option(\"dbtable\", \"fire_service_over_time\")\\\n",
    "      .option(\"user\", \"sahil\")\\\n",
    "      .option(\"password\",\"zxcvbnm\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "\n",
    "spark.read\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"<driver name>\")\\\n",
    "      .option(\"url\", \"jdbc:<source>://<ip>:<port>/<dbname>\")\\\n",
    "      .option(\"dbtable\", \"<table>\")\\\n",
    "      .option(\"user\", \"<username>\")\\\n",
    "      .option(\"password\",\"<password>\")\\\n",
    "      .load()\n",
    "\n",
    "#### Write\n",
    "\n",
    "df.write\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "      .option(\"url\", \"jdbc:postgresql://localhost:5432/spark_demo_db\")\\\n",
    "      .option(\"dbtable\", \"my_table\")\\\n",
    "      .option(\"user\", \"sahil\")\\\n",
    "      .option(\"password\",\"zxcvbnm\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MYSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "df.read\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "      .option(\"url\", \"jdbc:mysql://<ip>:3306/<dbname>\")\\\n",
    "      .option(\"dbtable\", \"<table>\")\\\n",
    "      .option(\"user\", \"<user>\")\\\n",
    "      .option(\"password\",\"<passwd>\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .load()\n",
    "\n",
    "\n",
    "\n",
    "#### Write\n",
    "df.write\\\n",
    "      .format(\"jdbc\")\\\n",
    "      .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "      .option(\"url\", \"jdbc:mysql://<i[>:3306/<dbname>\")\\\n",
    "      .option(\"dbtable\", \"<tablename>\")\\\n",
    "      .option(\"user\", \"<user>\")\\\n",
    "      .option(\"password\",\"<passwrd>\")\\\n",
    "      .mode(\"overwrite\")\\\n",
    "      .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install SQL Server from [here](https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu?view=sql-server-2017)\n",
    "\n",
    "Download latest driver package from [here](https://docs.microsoft.com/en-us/sql/connect/jdbc/download-microsoft-jdbc-driver-for-sql-server?view=sql-server-2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include the jar from the above downloaded packages as --driver-class-path option\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = = '--driver-class-path /home/sahil/Desktop/sqljdbc_7.2/enu/mssql-jdbc-7.2.1.jre8.jar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "    .option(\"url\", \"jdbc:sqlserver://<ip>:1433;databaseName=<dbname>\") \\\n",
    "    .option(\"dbtable\", \"<table>\") \\\n",
    "    .option(\"user\", \"<user>\") \\\n",
    "    .option(\"password\", \"<pasword>\")\\\n",
    "    .load()\n",
    "\n",
    "#### Write\n",
    "\n",
    "df.write.format(\"jdbc\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\")\\\n",
    "    .option(\"url\", \"jdbc:sqlserver://<ip>:1433;databaseName=<dbname>\") \\\n",
    "    .option(\"dbtable\", \"<table>\") \\\n",
    "    .option(\"user\", \"<user>\") \\\n",
    "    .option(\"password\", \"<password>\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-14T19:05:25.396811Z",
     "start_time": "2019-03-14T19:05:25.392092Z"
    }
   },
   "source": [
    "# NoSQL Databases\n",
    "\n",
    "|Source| Driver Package|Format Name|Standard Port|\n",
    "|-----------|----------------|---------|----|\n",
    "|Cassandra  |com.datastax.spark:spark-cassandra-connector_2.11:2.3.0|org.apache.spark.sql.cassandra|9042\n",
    "|DynamoDB   |com.amazon.emr:emr-dynamodb-hadoop:4.2.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "\n",
    "spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "          .option(\"spark.cassandra.connection.host\",\"<ip>\")\\\n",
    "          .option(\"spark.cassandra.connection.port\",\"<port>\")\\\n",
    "          .option(\"keyspace\",\"<keyspace name>\")\\\n",
    "          .option(\"table\",\"<table name>\")\n",
    "          .load()\n",
    "\n",
    "#### Write\n",
    "\n",
    "spark.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "          .option(\"spark.cassandra.connection.host\",\"<ip>\")\\\n",
    "          .option(\"spark.cassandra.connection.port\",\"<port>\")\\\n",
    "          .option(\"keyspace\",\"<keyspace name>\")\\\n",
    "          .option(\"table\",\"<table name>\")\n",
    "          .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamo DB (TODO)\n",
    "\n",
    "- I dont think it is possible to get data in/out from DynamoDB using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Read\n",
    "\n",
    "\n",
    "\n",
    "#### Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to Include Drivers and Jars\n",
    "\n",
    "Drivers from https://mvnrepository.com/\n",
    "\n",
    "Jar Format is **groupID:artifactID:version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 1\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName('postgres spark demo')\\\n",
    "                    .master(\"local\")\\\n",
    "                    .config(\"spark.jars.packages\", \"<jar>,<jar>\")\\\n",
    "                    .getOrCreate()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 2\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \\\n",
    "                                     <jar>,<jar> \\\n",
    "                                     pyspark-shell'\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName('Demo 1')\\\n",
    "                    .master(\"local\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:09:32.875256Z",
     "start_time": "2019-03-08T18:09:32.870852Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.<format>(\"s3a://<bucket name>/<file name>\")\n",
    "spark.write.<format>(\"s3a://<bucket name>/<file name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark Deployment Modes\n",
    "* Interactive  or   Cluser\n",
    "\n",
    "* Local Mode\n",
    "* Standalone Mode\n",
    "* Cluster Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:13:09.822593Z",
     "start_time": "2019-03-08T18:13:09.820025Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Local Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:13:31.233429Z",
     "start_time": "2019-03-08T18:13:31.229669Z"
    }
   },
   "source": [
    "* When you use master as local[2] you request Spark to use 2 core's and run the driver and workers in the same JVM. \n",
    "* In local mode all spark job related tasks run in the same JVM.\n",
    "* Its useful for learning purpose only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:13:31.233429Z",
     "start_time": "2019-03-08T18:13:31.229669Z"
    }
   },
   "source": [
    "![Notebook + Micro Cluster](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/book_intro/notebook_microcluster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:16:37.332296Z",
     "start_time": "2019-03-08T18:16:37.318757Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "spark = from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName('first app')\\\n",
    "                    .master(\"local[2]\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master local[4] my_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Standalone Cluster Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://zeppelin.apache.org/docs/0.7.0/assets/themes/zeppelin/img/docs-img/spark_ui.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cluster Manager inbuilt with spark. \n",
    "* Not used in production (YARN/MESOS/Kubernets)\n",
    "* Good for testing purposes, before deployment - does not support deploy mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:26:43.332202Z",
     "start_time": "2019-03-08T18:26:43.319259Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How to create a cluster on local machine?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start spark master\n",
    "$SPARK_HOME/sbin/start-master.sh -h localhost\n",
    "\n",
    "# start slave\n",
    "$SPARK_HOME/sbin/start-slave.sh spark://localhost:7077\n",
    "        \n",
    "# check master UI at \n",
    "htpps://localhost:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName('first app')\\\n",
    "                    .master(\"spark://localhost:7077\")\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark-submit --master spark://localhost:7077 my_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:33:55.291520Z",
     "start_time": "2019-03-08T18:33:55.288960Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cluster Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:35:15.055716Z",
     "start_time": "2019-03-08T18:35:15.046009Z"
    }
   },
   "outputs": [],
   "source": [
    "spark-submit --master <master ip> --deploy-mode cluster my_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook --no-browser --port=8889 --ip=\"*\"\"\n",
    "\n",
    "pyspark --master <master ip>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:43:59.765397Z",
     "start_time": "2019-03-08T18:43:59.763020Z"
    }
   },
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T18:45:01.726241Z",
     "start_time": "2019-03-08T18:45:01.722074Z"
    }
   },
   "source": [
    "Start the jupyter notebook as it is! Create the spark context inside the jupyter session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning & Analytics\n",
    "\n",
    "* ML Algorithms: common learning algorithms such as classification, regression, clustering, and collaborative filtering\n",
    "* Featurization: feature extraction, transformation, dimensionality reduction, and selection\n",
    "* Pipelines: tools for constructing, evaluating, and tuning ML Pipelines\n",
    "* Persistence: saving and load algorithms, models, and Pipelines\n",
    "* Utilities: linear algebra, statistics, data handling, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Cacheing and Repartitioning Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = df.repartition(8) ## according to number of executors\n",
    "df.createOrReplaceTempView(\"fireServiceVIEW\");\n",
    "spark.catalog.cacheTable(\"fireServiceVIEW\")\n",
    "df = spark.table(\"fireServiceVIEW\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
